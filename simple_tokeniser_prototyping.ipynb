{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4247b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import collections\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cff098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/tom/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "dataset_group = 'wikitext'\n",
    "dataset_name = 'wikitext-103-raw-v1'\n",
    "dataset_split = 'train'\n",
    "dataset_full_name = '/'.join([dataset_group, dataset_name, dataset_split])\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_group, name=dataset_name, split=dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133a1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars_and_counts(dataset):\n",
    "    \"\"\"Takes a HF dataset and returns a dictionary {char: count}.\"\"\"\n",
    "    char_counts = collections.defaultdict(int)\n",
    "    for record in tqdm.tqdm(dataset):\n",
    "        for char in record['text']:\n",
    "            char_counts[char] += 1\n",
    "            \n",
    "    return char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258f2d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1801350/1801350 [00:30<00:00, 59648.54it/s]\n"
     ]
    }
   ],
   "source": [
    "char_counts = get_chars_and_counts(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f41c44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_by_count = [(char, char_counts[char]) for char in char_counts]\n",
    "chars_by_count = sorted(chars_by_count, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c989842c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 102591407),\n",
       " ('e', 49091225),\n",
       " ('t', 34095601),\n",
       " ('a', 33724387),\n",
       " ('n', 29289422),\n",
       " ('i', 28992651),\n",
       " ('o', 28818255),\n",
       " ('r', 26331592),\n",
       " ('s', 25200925),\n",
       " ('h', 19044643)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_by_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "705e1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = ['[PAD]', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab4003cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1024\n",
    "num_nonspecial_chars = vocab_size - len(special_chars)\n",
    "\n",
    "chars_to_keep = [(char, 0) for char in special_chars] + chars_by_count[:num_nonspecial_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8058935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicts to map both ways\n",
    "char_to_id = {}\n",
    "id_to_char = {}\n",
    "\n",
    "for i, (char, _) in enumerate(chars_to_keep):\n",
    "    char_to_id[char] = i\n",
    "    id_to_char[i] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34d9c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(data, max_seq_len=512):\n",
    "    tokens = []\n",
    "    seq_len = 0\n",
    "    \n",
    "    # Tokenise\n",
    "    for char in data:\n",
    "        if char in char_to_id:\n",
    "            tok = char_to_id[char]\n",
    "        else:\n",
    "            tok = char_to_id['[UNK]']\n",
    "            \n",
    "        tokens.append(tok)\n",
    "        seq_len += 1\n",
    "        \n",
    "        if seq_len > max_seq_len:\n",
    "            raise ValueError('Sequence to tokenise exceeds length limit.')\n",
    "            \n",
    "    # Pad\n",
    "    num_to_pad = max_seq_len - seq_len\n",
    "    pad_tok = char_to_id['[PAD]']\n",
    "    tokens += [pad_tok] * num_to_pad\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9997f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lipsum = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed lectus nulla, pulvinar sed auctor nec, facilisis eu odio. Duis euismod pellentesque turpis, vitae ullamcorper tortor rutrum quis. Duis sed odio ut augue convallis convallis. Morbi at elit ut mi imperdiet vehicula. Suspendisse in sem eget est dapibus pellentesque. In ut condimentum purus. Vivamus vulputate est massa, id pretium quam pharetra eget. Duis porta ipsum vitae nibh tempus, eu ultricies nunc molestie. Nulla facilisi. Donec eu erat vitae leo laoreet mollis a quis metus. In eu libero porta magna vehicula venenatis. Praesent fermentum quam libero, ac volutpat dui tincidunt ac. Pellentesque vitae risus viverra, rhoncus augue ut, pellentesque dui.\"\"\"\n",
    "tokenised = tokenise(lipsum[:508])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9ee2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenise(data):\n",
    "    return ''.join([id_to_char[tok] for tok in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f976ea6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed lectus nulla, pulvinar sed auctor nec, facilisis eu odio. Duis euismod pellentesque turpis, vitae ullamcorper tortor rutrum quis. Duis sed odio ut augue convallis convallis. Morbi at elit ut mi imperdiet vehicula. Suspendisse in sem eget est dapibus pellentesque. In ut condimentum purus. Vivamus vulputate est massa, id pretium quam pharetra eget. Duis porta ipsum vitae nibh tempus, eu ultricies nunc molestie. Nulla facilisi. Donec eu erat vita[PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenise(tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e3b8a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed lectus nulla, pulvinar sed auctor nec, facilisis eu odio. Duis euismod pellentesque turpis, vitae ullamcorper tortor rutrum quis. Duis sed odio ut augue convallis convallis. Morbi at elit ut mi imperdiet vehicula. Suspendisse in sem eget est dapibus pellentesque. In ut condimentum purus. Vivamus vulputate est massa, id pretium quam pharetra eget. Duis porta ipsum vitae nibh tempus, eu ultricies nunc molestie. Nulla facilisi. Donec eu erat vitae le'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipsum[:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3177c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
