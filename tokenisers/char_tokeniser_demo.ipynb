{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d315236",
   "metadata": {},
   "source": [
    "# Character tokeniser demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ae8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f510421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import char_tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb0f52",
   "metadata": {},
   "source": [
    "First, instantiate the tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8b469e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = char_tokeniser.CharacterTokeniser(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18c089",
   "metadata": {},
   "source": [
    "Load the wikitext dataset to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b467c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/tom/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "dataset_group = 'wikitext'\n",
    "dataset_name = 'wikitext-103-raw-v1'\n",
    "dataset_split = 'train'\n",
    "dataset_full_name = '/'.join([dataset_group, dataset_name, dataset_split])\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_group, name=dataset_name, split=dataset_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8da24",
   "metadata": {},
   "source": [
    "Train the tokeniser on the wikitext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24826377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1801350/1801350 [00:28<00:00, 62808.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tokeniser.train(dataset)  # takes ~30s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a5dd5",
   "metadata": {},
   "source": [
    "Check tokenisation and detokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278afd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lipsum = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed lectus nulla, pulvinar sed auctor nec, facilisis eu odio. Duis euismod pellentesque turpis, vitae ullamcorper tortor rutrum quis. Duis sed odio ut augue convallis convallis. Morbi at elit ut mi imperdiet vehicula. Suspendisse in sem eget est dapibus pellentesque. In ut condimentum purus. Vivamus vulputate est massa, id pretium quam pharetra eget. Duis porta ipsum vitae nibh tempus, eu ultricies nunc molestie. Nulla facilisi. Donec eu erat vitae leo laoreet mollis a quis metus. In eu libero porta magna vehicula venenatis. Praesent fermentum quam libero, ac volutpat dui tincidunt ac. Pellentesque vitae risus viverra, rhoncus augue ut, pellentesque dui.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e78a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised = tokeniser.tokenise(lipsum[:508], max_seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a80d036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed lectus nulla, pulvinar sed auctor nec, facilisis eu odio. Duis euismod pellentesque turpis, vitae ullamcorper tortor rutrum quis. Duis sed odio ut augue convallis convallis. Morbi at elit ut mi imperdiet vehicula. Suspendisse in sem eget est dapibus pellentesque. In ut condimentum purus. Vivamus vulputate est massa, id pretium quam pharetra eget. Duis porta ipsum vitae nibh tempus, eu ultricies nunc molestie. Nulla facilisi. Donec eu erat vita[PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.detokenise_to_string(tokenised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7c17f",
   "metadata": {},
   "source": [
    "How fast does it tokenise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e6fa92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 1000000/1000000 [00:32<00:00, 30619.31it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(range(int(1e6))):\n",
    "    tokenised = tokeniser.tokenise(lipsum[:508], max_seq_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97feb3fa",
   "metadata": {},
   "source": [
    "So it's hardly a speed demon but isn't going to be the bottleneck in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bb1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
